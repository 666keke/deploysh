import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import category_encoders as ce
import itertools # Not strictly used in final app but was in original
from scipy.stats import chi2_contingency

# --- Page Configuration ---
st.set_page_config(layout="wide", page_title="ä¸Šæµ·è¯æ•°æ®äº¤äº’åˆ†æå¹³å°")
st.title("ğŸ“Š ä¸Šæµ·è¯é—®å·æ•°æ®äº¤äº’å¼åˆ†æå¹³å°")
st.markdown("""
æ¬¢è¿ä½¿ç”¨ä¸Šæµ·è¯é—®å·æ•°æ®äº¤äº’åˆ†æå¹³å°ï¼
è¯·åœ¨å·¦ä¾§è¾¹æ é€‰æ‹©ç­›é€‰æ¡ä»¶ã€åˆ†ææŒ‡æ ‡å’Œåˆ†ç»„æ–¹å¼ï¼Œå³ä¾§å°†å±•ç¤ºç›¸åº”çš„æ•°æ®å›¾è¡¨å’Œè¡¨æ ¼ã€‚
""")

# --- Data Loading and Preprocessing (Adapted from share.py) ---
@st.cache_data # Cache the data loading and processing
def load_and_process_data():
    df = pd.read_csv('data.csv')

    # Initial filter as in share.py
    # Consider making this an optional filter in the UI if broader analysis is needed
    df = df[df['16.ä½ æ˜¯å¦æ„¿æ„ä¸ºäº†ä¼ æ‰¿æ–‡åŒ–å»ç‰¹æ„å­¦ä¹ ä¸Šæµ·è¯ï¼Ÿ'] == 'D.æ— æ‰€è°“ï¼ˆè¯·ç›´æ¥é€‰æ‹©æ­¤é¡¹ï¼‰'].copy() # Use .copy() to avoid SettingWithCopyWarning

    drop_cols = [
        'ç¼–å·', 'å¼€å§‹ç­”é¢˜æ—¶é—´', 'ç»“æŸç­”é¢˜æ—¶é—´', 'ç­”é¢˜æ—¶é•¿',
        '3.ä½ æ¥è‡ªäºï¼š_å¡«ç©º3',
        'è¯­è¨€', 'æ¸…æ´—æ•°æ®ç»“æœ', 'æ™ºèƒ½æ¸…æ´—æ•°æ®æ— æ•ˆæ¦‚ç‡',
        'åœ°ç†ä½ç½®å›½å®¶å’Œåœ°åŒº', 'åœ°ç†ä½ç½®çœ', 'åœ°ç†ä½ç½®å¸‚',
        'ç”¨æˆ·ç±»å‹', 'ç”¨æˆ·æ ‡è¯†', 'æ˜µç§°', 'è‡ªå®šä¹‰å­—æ®µ', 'IP', 'UA',
        'Referrer', 'ä¸­å¥–æ—¶é—´', 'ä¸­å¥–é‡‘é¢', 'å®¡æ ¸çŠ¶æ€', 'Unnamed: 58',
        '16.ä½ æ˜¯å¦æ„¿æ„ä¸ºäº†ä¼ æ‰¿æ–‡åŒ–å»ç‰¹æ„å­¦ä¹ ä¸Šæµ·è¯ï¼Ÿ'
    ]
    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)

    multi_cols_21 = [c for c in df.columns if c.startswith('21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯')]
    multi_cols_24 = [c for c in df.columns if c.startswith('24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹')]
    for col in multi_cols_21 + multi_cols_24:
        df[col] = df[col].notna().astype(int)

    rename_map = {
        '1.ä½ çš„æ€§åˆ«æ˜¯ï¼Ÿ': 'gender',
        '2.ä½ çš„å¹´çº§æ˜¯ï¼Ÿ': 'grade',
        '4.ä½ çš„ä¸“ä¸šç±»å‹æ˜¯ï¼Ÿ': 'major',
        '5.ä½ ç›®å‰å°±è¯»çš„å­¦æ ¡æ˜¯ï¼Ÿ':'school',
        '6.ä½ æ˜¯å¦ä¸ºä¸Šæµ·æœ¬åœ°äººï¼Ÿ': 'native',
        '7.ä½ çš„çˆ¶æ¯æ˜¯å¦ä¸ºä¸Šæµ·æœ¬åœ°äººï¼Ÿ':'parents',
        '8.ä½ å¯¹ä¸Šæµ·è¯çš„æ•´ä½“å°è±¡æ˜¯ï¼Ÿ': 'overall_impression',
        '9.ä½ è®¤ä¸ºä¸Šæµ·è¯å±äºä¸€ç§ï¼š': 'shanghainese_attitude',
        '10.ä½ è®¤ä¸ºå­¦ä¹ /ä¼šè¯´ä¸Šæµ·è¯æ˜¯å¦æ˜¯ä¸€ç§â€œæœ¬åœ°èº«ä»½â€çš„è±¡å¾ï¼Ÿ': 'identity',
        '14.ä½ å¯¹åœ¨å…¬å…±åœºåˆå¬åˆ°ä¸Šæµ·è¯çš„çœ‹æ³•æ˜¯ï¼Ÿ': 'public_hear_view',
        '15.ä½ å¯¹ç§äººåœºåˆä½¿ç”¨ä¸Šæµ·è¯äº¤æµçš„çœ‹æ³•æ˜¯ï¼Ÿ': 'private_use_view',
        '11.ä½ å¯¹å¤§å­¦ä¸­å¼€è®¾ä¸Šæµ·è¯è¯¾ç¨‹çš„æ€åº¦æ˜¯ï¼Ÿ': 'course_attitude',
        '12.ä½ æ˜¯å¦è®¤åŒâ€œå¹´è½»ä¸€ä»£åº”è¯¥ä¼šè¯´ä¸€äº›ä¸Šæµ·è¯â€ï¼Ÿ':'young_should',
        '13.ä½ è§‰å¾—ä¸Šæµ·è¯åœ¨ç°ä»£ç¤¾ä¼šä¸­çš„åœ°ä½æ˜¯ï¼Ÿ':'social_status_of_shanghainese',
        '17.ä½ æ˜¯å¦è§‰å¾—å­¦æ ¡æˆ–ç¤¾ä¼šåº”è¯¥æä¾›æ›´å¤šå­¦ä¹ ä¸Šæµ·è¯çš„æœºä¼šï¼Ÿ': 'more_learning_opportunity',
        '18.ä½ æ˜¯å¦ä¼šè¯´ä¸Šæµ·è¯ï¼Ÿ': 'speaking_ability',
        '19.ä½ èƒ½å¬æ‡‚ä¸Šæµ·è¯çš„ç¨‹åº¦æ˜¯ï¼Ÿ': 'listening_ability',
        '20.ä½ ä¸å®¶äººäº¤æµæ—¶æœ€å¸¸ç”¨çš„è¯­è¨€æ˜¯ï¼Ÿ':'family_language',
        '21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯ï¼Ÿ:ä¸å®¶äººäº¤æµ':'family_use',
        '21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯ï¼Ÿ:ä¸æœ‹å‹äº¤æµ':'friend_use',
        '21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯ï¼Ÿ:åœ¨æœ¬åœ°ç¤¾åŒºæˆ–é‚»é‡Œé—´':'local_community_use',
        '21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯ï¼Ÿ:åœ¨å·¥ä½œ/å…¼èŒä¸­':'work_use',
        '21.ä½ é€šå¸¸åœ¨ä»¥ä¸‹å“ªäº›åœºåˆä½¿ç”¨ä¸Šæµ·è¯ï¼Ÿ:åŸºæœ¬ä¸ç”¨':'basic_no_use',
        '22.ä½ ä½¿ç”¨ä¸Šæµ·è¯çš„é¢‘ç‡æ˜¯ï¼Ÿ': 'usage_freq',
        '23.ä½ ä¼šä½¿ç”¨ä¸Šæµ·è¯å‘å¾®ä¿¡/ç¤¾äº¤å¹³å°ä¿¡æ¯å—ï¼Ÿ': 'sns_use_freq',
        '24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹ï¼Ÿ:ä¸Šæµ·è¯é…éŸ³çŸ­è§†é¢‘':'shanghainese_dubbling_tiktok',
        '24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹ï¼Ÿ:ä¸Šæµ·è¯ç”µè§†å‰§/ç”µå½±':'shanghainese_movies',
        '24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹ï¼Ÿ:ä¸Šæµ·è¯å¹¿æ’­/éŸ³é¢‘èŠ‚ç›®':'shanghainese_radio',
        '24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹ï¼Ÿ:ä¸Šæµ·è¯å­¦ä¹ ç±»å†…å®¹':'shanghainese_learning_resources',
        '24.ä½ çœ‹è¿‡æˆ–å¬è¿‡ä»¥ä¸‹å“ªç±»ä¸ä¸Šæµ·è¯æœ‰å…³çš„å†…å®¹ï¼Ÿ:å‡ ä¹æ²¡æœ‰æ¥è§¦è¿‡':'never_shanghainese_content',
        '25.ä½ æ˜¯å¦å…³æ³¨è¿‡æ²ªè¯­åšä¸»ï¼ˆå¦‚Gåƒ§ä¸œã€å†Œé‚£é˜Ÿé•¿ç­‰ï¼‰ï¼Ÿ': 'follow_sh_blogger',
        '26.ä½ å¯¹æ­¤ç±»æ²ªè¯­è§†é¢‘çš„çœ‹æ³•æ˜¯ï¼Ÿ': 'video_view',
        '27.ä½ æ˜¯å¦æ›¾å› ä¸ä¼šè¯´ä¸Šæµ·è¯è€Œæ„Ÿåˆ°å°´å°¬/è¢«æ’æ–¥ï¼Ÿ': 'awkward_score',
        '28.ä½ è®¤ä¸ºç›®å‰çš„è¯­è¨€ç¯å¢ƒæ˜¯å¦æ”¯æŒä¸Šæµ·è¯çš„ä½¿ç”¨ï¼Ÿ': 'env_support',
        '3.ä½ æ¥è‡ªäºï¼š_å¡«ç©º1': 'Province',
        '3.ä½ æ¥è‡ªäºï¼š_å¡«ç©º2': 'City',
    }
    df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)

    def clean_province(name):
        suffixes = ['çœ', 'å¸‚', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº', 'å›æ—', 'å£®æ—', 'ç»´å¾å°”', 'ç»´å¾å°”æ—']
        for suffix in suffixes:
            if isinstance(name, str) and name.endswith(suffix):
                return name[:-len(suffix)]
        return name

    province_to_region = {
        'ä¸Šæµ·': 'ä¸Šæµ·æœ¬åœ°', 'æ±Ÿè‹': 'åä¸œ', 'æµ™æ±Ÿ': 'åä¸œ', 'å®‰å¾½': 'åä¸œ', 'ç¦å»º': 'åä¸œ', 'å±±ä¸œ': 'åä¸œ', 'æ±Ÿè¥¿': 'åä¸œ',
        'å¹¿ä¸œ': 'åå—', 'å¹¿è¥¿': 'åå—', 'æµ·å—': 'åå—',
        'åŒ—äº¬': 'ååŒ—', 'å¤©æ´¥': 'ååŒ—', 'æ²³åŒ—': 'ååŒ—', 'å±±è¥¿': 'ååŒ—', 'å†…è’™å¤': 'ååŒ—',
        'å››å·': 'è¥¿å—', 'é‡åº†': 'è¥¿å—', 'äº‘å—': 'è¥¿å—', 'è´µå·': 'è¥¿å—', 'è¥¿è—': 'è¥¿å—',
        'é™•è¥¿': 'è¥¿åŒ—', 'ç”˜è‚ƒ': 'è¥¿åŒ—', 'é’æµ·': 'è¥¿åŒ—', 'å®å¤': 'è¥¿åŒ—', 'æ–°ç–†': 'è¥¿åŒ—',
        'è¾½å®': 'ä¸œåŒ—', 'å‰æ—': 'ä¸œåŒ—', 'é»‘é¾™æ±Ÿ': 'ä¸œåŒ—'
    }

    if 'City' in df.columns and 'Province' in df.columns:
        df.loc[df['City'].isin(['ä¸Šæµ·', 'ä¸Šæµ·å¸‚']), 'Province'] = 'ä¸Šæµ·'
        df.loc[df['City'].isin(['åŒ—äº¬', 'åŒ—äº¬å¸‚']), 'Province'] = 'åŒ—äº¬'
        df.loc[df['City'].isin(['å¤©æ´¥', 'å¤©æ´¥å¸‚']), 'Province'] = 'å¤©æ´¥'
        df.loc[df['City'].isin(['é‡åº†', 'é‡åº†å¸‚']), 'Province'] = 'é‡åº†'

        df['Province'] = df['Province'].apply(clean_province)
        df['Province'] = df['Province'].apply(clean_province) # Apply twice as in original
        df['Region'] = df['Province'].map(province_to_region).fillna('å…¶ä»–åœ°åŒº')

    # Specific Encodings for analysis & creating string columns for filters
    gender_mapping_display = {1: 'ç”·', 0: 'å¥³', 2:'å…¶ä»–'} # For display
    if 'gender' in df.columns:
        df['gender_code'] = df['gender'].map({'A.ç”·':1, 'B.å¥³':0, 'C.å…¶ä»–':2})
        df['gender_str'] = df['gender_code'].map(gender_mapping_display)

    native_mapping_display = {'A.æ˜¯ï¼Œåœ¨ä¸Šæµ·å‡ºç”Ÿå¹¶é•¿å¤§': 'ä¸Šæµ·æœ¬åœ°äºº(å‡ºç”Ÿå¹¶é•¿å¤§)',
                              'B.å¦ï¼Œä½†åœ¨ä¸Šæµ·ç”Ÿæ´»è¶…è¿‡5å¹´': 'é•¿æœŸå±…ä½ä¸Šæµ·(>5å¹´)',
                              'C.å¦ï¼Œåœ¨ä¸Šæµ·ç”Ÿæ´»ä¸è¶³5å¹´': 'çŸ­æœŸå±…ä½ä¸Šæµ·(<5å¹´)'}
    if 'native' in df.columns:
        df['native_flag'] = (df['native'] == 'A.æ˜¯ï¼Œåœ¨ä¸Šæµ·å‡ºç”Ÿå¹¶é•¿å¤§').astype(int)
        df['long_term_sh'] = (df['native'] == 'B.å¦ï¼Œä½†åœ¨ä¸Šæµ·ç”Ÿæ´»è¶…è¿‡5å¹´').astype(int)
        df['native_str'] = df['native'].map(lambda x: native_mapping_display.get(x, x))


    if 'follow_sh_blogger' in df.columns:
        df['follow_sh_blogger'] = df['follow_sh_blogger'].map({'A.æ˜¯':1, 'B.å¦':0})
    if 'identity' in df.columns:
        df['identity'] = df['identity'].map({'A.æ˜¯çš„':4, 'B.éƒ¨åˆ†æ˜¯':3, 'D.ä¸æ¸…æ¥š':2, 'C.å¦':1})
    if 'course_attitude' in df.columns:
        df['course_attitude'] = df['course_attitude'].map({'A.éå¸¸æ”¯æŒ':4, 'B.æ”¯æŒ':3, 'C.æ— æ‰€è°“':2, 'D.åå¯¹':1})
    if 'young_should' in df.columns:
        df['young_should'] = df['young_should'].map({'A.éå¸¸è®¤åŒ':4, 'B.è®¤åŒ':3, 'C.ä¸å¤ªè®¤åŒ':2, 'D.å®Œå…¨ä¸è®¤åŒ':1})
    if 'shanghainese_attitude' in df.columns:
        df['shanghainese_attitude'] = df['shanghainese_attitude'].map({'A.åœ°æ–¹è¯­è¨€ï¼Œåº”äºˆä¿æŠ¤':4, 'B.æ²Ÿé€šå·¥å…·ï¼Œå®ç”¨å³å¯':3, 'D.æ— æ‰€è°“':2, 'C.æ–¹è¨€ï¼Œé€æ¸æ¶ˆå¤±æ˜¯è‡ªç„¶ç°è±¡':1})
    if 'social_status_of_shanghainese' in df.columns:
        df['social_status_of_shanghainese'] = df['social_status_of_shanghainese'].map({'A.é‡è¦ï¼Œåº”é‡è§†':4, 'B.ä¸€èˆ¬ï¼Œå¯ä¿ç•™å¯å–ä»£':3, 'D.éš¾è¯´':2, 'C.ä¸é‡è¦':1})
    if 'more_learning_opportunity' in df.columns:
        df['more_learning_opportunity'] = df['more_learning_opportunity'].map({'A.æ˜¯':1, 'B.å¦':-1, 'C.æ— æ‰€è°“': 0})
    if 'awkward_score' in df.columns:
        df['awkward_score'] = pd.to_numeric(df['awkward_score'], errors='coerce').fillna(0) # Ensure numeric before negation
        df['awkward_score'] = -df['awkward_score']

    original_major_col = df['major'].copy() if 'major' in df.columns else pd.Series()
    original_grade_col = df['grade'].copy() if 'grade' in df.columns else pd.Series()

    cat_cols = df.select_dtypes(include=['object']).columns.tolist()
    # Columns to keep as strings for direct use (not to be ordinally encoded here if present)
    keep_as_string = ['Region', 'gender_str', 'native_str', 'Province', 'City', 'school', 'parents', 'family_language', 'gender', 'native']
    cat_cols_for_encoding = [col for col in cat_cols if col not in keep_as_string]

    major_mapping = {}
    grade_mapping = {}

    if cat_cols_for_encoding:
        encoder = ce.OrdinalEncoder(cols=cat_cols_for_encoding, handle_unknown='value', handle_missing='return_nan') # Use return_nan for explicit handling
        df = encoder.fit_transform(df)

        for mapping_info in encoder.category_mapping:
            col_name = mapping_info['col']
            current_map = {k:v for k,v in mapping_info['mapping'].items() if not (isinstance(k, float) and np.isnan(k))}
            if col_name == 'major':
                major_mapping = current_map
            elif col_name == 'grade':
                grade_mapping = current_map

    # Fallback or ensure major/grade mappings exist even if not in cat_cols_for_encoding (e.g. if already numeric by mistake)
    if not major_mapping and 'major' in df.columns:
        unique_majors = original_major_col.dropna().unique()
        major_mapping = {val: i+1 for i, val in enumerate(unique_majors)}
        df['major'] = original_major_col.map(major_mapping) # Re-map using original values if needed

    if not grade_mapping and 'grade' in df.columns:
        unique_grades = original_grade_col.dropna().unique()
        grade_mapping = {val: i+1 for i, val in enumerate(unique_grades)}
        df['grade'] = original_grade_col.map(grade_mapping) # Re-map

    # Define attitude_cols (inferred and explicit)
    attitude_cols = [
        'overall_impression', 'shanghainese_attitude', 'identity',
        'public_hear_view', 'private_use_view', 'course_attitude',
        'young_should', 'social_status_of_shanghainese',
        'more_learning_opportunity', 'video_view', 'awkward_score', 'env_support'
    ]
    # Filter out cols not in df from attitude_cols
    attitude_cols = [col for col in attitude_cols if col in df.columns]

    question_cols_list = attitude_cols + [
        'speaking_ability', 'listening_ability', 'usage_freq', 'sns_use_freq',
        'family_use', 'friend_use', 'local_community_use', 'work_use', 'basic_no_use',
        'shanghainese_dubbling_tiktok', 'shanghainese_movies', 'shanghainese_radio',
        'shanghainese_learning_resources', 'never_shanghainese_content',
        'follow_sh_blogger'
    ]

    final_question_cols = []
    for col in question_cols_list:
        if col in df.columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                df[col] = pd.to_numeric(df[col], errors='coerce')
            # Drop rows where essential numeric question columns became NaN after conversion, or fill them
            # df.dropna(subset=[col], inplace=True) # This might reduce data significantly
            # For now, we let mean handle NaNs, but this is a point of attention for data quality.
            final_question_cols.append(col)
    final_question_cols = sorted(list(set(final_question_cols))) # Unique and sorted

    # Create string versions for major and grade for grouping display using the mappings
    # Mappings are {original_name: encoded_code}
    rev_major_mapping = {v: k for k, v in major_mapping.items()} if major_mapping else {}
    rev_grade_mapping = {v: k for k, v in grade_mapping.items()} if grade_mapping else {}

    if 'major' in df.columns and major_mapping:
      df['major_str'] = df['major'].map(rev_major_mapping).fillna('æœªçŸ¥')
    if 'grade' in df.columns and grade_mapping:
      df['grade_str'] = df['grade'].map(rev_grade_mapping).fillna('æœªçŸ¥')

    return df, final_question_cols, rename_map, major_mapping, grade_mapping, gender_mapping_display, native_mapping_display, province_to_region

df_processed, question_cols, rename_map, major_mapping, grade_mapping, gender_map_disp, native_map_disp, prov_to_region_map = load_and_process_data()

# --- Sidebar for Controls ---
st.sidebar.header("âš™ï¸ ç­›é€‰ä¸å¯è§†åŒ–é€‰é¡¹")

# Metrics and Grouping Selection
rev_rename_map = {v: k for k, v in rename_map.items()}
def get_display_name(q_col):
    original_question = rev_rename_map.get(q_col, q_col)
    # Simplify common question formats
    name = original_question.split("ï¼Ÿ:")[-1].split('ï¼Ÿ')[-1].split('æ˜¯ï¼š')[-1]
    name = name.replace('ï¼ˆè¯·ç›´æ¥é€‰æ‹©æ­¤é¡¹ï¼‰','').replace("ï¼ˆ", "(").replace("ï¼‰", ")").strip()
    if len(name) > 50: # Truncate very long names
        name = name[:47] + "..."
    return name if name else q_col

question_cols_display_names = {}
if question_cols:
    question_cols_display_names = {q_col: q_col for q_col in question_cols}

# Initialize threshold analysis variables
threshold_metric = None
threshold_values = []

# Create filter options
major_filter_options = []
if major_mapping:
    major_filter_options = sorted([k for k in major_mapping.keys() if not (isinstance(k, float) and np.isnan(k))])
selected_major_names = st.sidebar.multiselect("ğŸ“ é€‰æ‹©ä¸“ä¸šç±»å‹", options=major_filter_options, default=major_filter_options)
selected_major_codes = [major_mapping[name] for name in selected_major_names if name in major_mapping]

grade_filter_options = []
if grade_mapping:
    grade_filter_options = sorted([k for k in grade_mapping.keys() if not (isinstance(k, float) and np.isnan(k))])
selected_grade_names = st.sidebar.multiselect("ğŸ“ˆ é€‰æ‹©å¹´çº§", options=grade_filter_options, default=grade_filter_options)
selected_grade_codes = [grade_mapping[name] for name in selected_grade_names if name in grade_mapping]

region_options = []
if 'Region' in df_processed.columns:
    region_options = sorted(df_processed['Region'].dropna().unique().tolist())
selected_regions = st.sidebar.multiselect("ğŸ—ºï¸ é€‰æ‹©åœ°åŒº", options=region_options, default=region_options)

gender_str_options = []
if 'gender_str' in df_processed.columns:
    gender_str_options = sorted(df_processed['gender_str'].dropna().unique().tolist())
selected_genders_str = st.sidebar.multiselect("ğŸš» é€‰æ‹©æ€§åˆ«", options=gender_str_options, default=gender_str_options)

native_str_options = []
if 'native_str' in df_processed.columns:
    native_str_options = sorted(df_processed['native_str'].dropna().unique().tolist())
selected_natives_str = st.sidebar.multiselect("ğŸ  é€‰æ‹©ä¸Šæµ·äººèº«ä»½", options=native_str_options, default=native_str_options)

# Apply filters
filtered_df = df_processed.copy()
if selected_major_codes and 'major' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['major'].isin(selected_major_codes)]
if selected_grade_codes and 'grade' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['grade'].isin(selected_grade_codes)]
if selected_regions and 'Region' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['Region'].isin(selected_regions)]
if selected_genders_str and 'gender_str' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['gender_str'].isin(selected_genders_str)]
if selected_natives_str and 'native_str' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['native_str'].isin(selected_natives_str)]


# Metrics and Grouping Selection

selected_metrics_keys = st.sidebar.multiselect(
    "ğŸ“Š é€‰æ‹©åˆ†ææŒ‡æ ‡ (Yè½´)",
    options=list(question_cols_display_names.keys()),
    format_func=lambda x: question_cols_display_names[x],
    default=[question_cols[0]] if question_cols else []
)

# Grouping variables (use string versions for display)
grouping_options_map = {}
if 'Region' in filtered_df.columns:
    grouping_options_map['Region'] = "åœ°åŒº"
if 'gender_str' in filtered_df.columns:
    grouping_options_map['gender_str'] = "æ€§åˆ«"
if 'native_str' in filtered_df.columns:
    grouping_options_map['native_str'] = "ä¸Šæµ·äººèº«ä»½"
if 'major_str' in filtered_df.columns and filtered_df['major_str'].nunique() > 0 :
    grouping_options_map['major_str'] = "ä¸“ä¸šç±»å‹"
if 'grade_str' in filtered_df.columns and filtered_df['grade_str'].nunique() > 0:
    grouping_options_map['grade_str'] = "å¹´çº§"


if grouping_options_map:
    selected_group_by_key = st.sidebar.selectbox(
        "ğŸ—‚ï¸ é€‰æ‹©åˆ†ç»„æ¡ä»¶ (Xè½´)",
        options=list(grouping_options_map.keys()),
        format_func=lambda x: grouping_options_map[x],
        index=0
    )
else:
    st.sidebar.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„åˆ†ç»„æ¡ä»¶ã€‚è¯·æ£€æŸ¥æ•°æ®ã€‚")
    selected_group_by_key = None


# --- Main Area for Charts and Tables ---
if not selected_metrics_keys or not selected_group_by_key or filtered_df.empty:
    st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†ææŒ‡æ ‡å’Œä¸€ä¸ªåˆ†ç»„æ¡ä»¶ï¼Œå¹¶ç¡®ä¿ç­›é€‰ç»“æœä¸ä¸ºç©ºã€‚")
    if filtered_df.empty:
        st.info("å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®ã€‚è¯·å°è¯•è°ƒæ•´ç­›é€‰å™¨ã€‚")
else:
    st.subheader("ğŸ“ˆ å›¾è¡¨åˆ†æ")

    for metric_key in selected_metrics_keys:
        metric_display_name = question_cols_display_names.get(metric_key, metric_key)
        group_by_display_name = grouping_options_map.get(selected_group_by_key, selected_group_by_key)

        if metric_key not in filtered_df.columns:
            st.error(f"æŒ‡æ ‡ '{metric_display_name}' ({metric_key}) åœ¨ç­›é€‰åçš„æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚")
            continue
        if selected_group_by_key not in filtered_df.columns:
            st.error(f"åˆ†ç»„æ¡ä»¶ '{group_by_display_name}' ({selected_group_by_key}) åœ¨ç­›é€‰åçš„æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚")
            continue

        try:
            if not pd.api.types.is_numeric_dtype(filtered_df[metric_key]):
                 st.error(f"æŒ‡æ ‡ '{metric_display_name}' ({metric_key}) ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œæ— æ³•è®¡ç®—å‡å€¼ã€‚")
                 continue

            # Drop NA for the specific metric and group_by col before grouping to avoid errors with all-NA groups
            temp_plot_df = filtered_df[[selected_group_by_key, metric_key]].dropna(subset=[metric_key, selected_group_by_key])
            if temp_plot_df.empty:
                st.info(f"æŒ‡æ ‡ '{metric_display_name}' æŒ‰ '{group_by_display_name}' åˆ†ç»„åæ— æœ‰æ•ˆæ•°æ®å¯ä¾›ç»˜å›¾ã€‚")
                continue

            plot_df = temp_plot_df.groupby(selected_group_by_key, as_index=False)[metric_key].mean()
            plot_df = plot_df.sort_values(by=metric_key, ascending=False)

            if plot_df.empty:
                st.info(f"æŒ‡æ ‡ '{metric_display_name}' æŒ‰ '{group_by_display_name}' åˆ†ç»„åæ— æ•°æ®å¯ä¾›ç»˜å›¾ã€‚")
                continue

            fig_title = f"'{metric_display_name}' æŒ‰ '{group_by_display_name}' åˆ†å¸ƒ (å‡å€¼)"
            fig = px.bar(plot_df, x=selected_group_by_key, y=metric_key,
                         title=fig_title,
                         labels={metric_key: f"å‡å€¼ - {metric_display_name}", selected_group_by_key: group_by_display_name},
                         color=selected_group_by_key,
                         text_auto='.2f')
            fig.update_layout(
                xaxis_title=group_by_display_name,
                yaxis_title=f"å‡å€¼ - {metric_display_name}",
                title_x=0.5,
                legend_title_text=group_by_display_name
            )
            st.plotly_chart(fig, use_container_width=True)

            csv_fig_data = plot_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½å›¾è¡¨ '{metric_display_name}' æ•°æ® (CSV)",
                data=csv_fig_data,
                file_name=f"{metric_key}_by_{selected_group_by_key}.csv",
                mime='text/csv',
                key=f"download_chart_{metric_key}_{selected_group_by_key}"
            )
            st.markdown("---")

        except Exception as e:
            st.error(f"ä¸ºæŒ‡æ ‡ '{metric_display_name}' å’Œåˆ†ç»„ '{group_by_display_name}' ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")


    st.subheader("ğŸ“„ ç­›é€‰åæ•°æ®é¢„è§ˆ (å‰100æ¡)")
    display_cols = []
    if selected_group_by_key:
        display_cols.append(selected_group_by_key)
    display_cols.extend(selected_metrics_keys)
    display_cols.extend([col for col in ['major_str', 'grade_str', 'Region', 'gender_str', 'native_str']
                    if col != selected_group_by_key and col in filtered_df.columns])
    st.dataframe(filtered_df[list(dict.fromkeys(display_cols))].head(100))

    csv_filtered_data = filtered_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½ç­›é€‰åå®Œæ•´æ•°æ® (CSV)",
        data=csv_filtered_data,
        file_name="filtered_shanghainese_data.csv",
        mime='text/csv',
        key="download_filtered_all"
    )

    # --- Threshold Analysis Section ---
    if not selected_metrics_keys or not selected_group_by_key or filtered_df.empty:
        pass  # Don't show threshold analysis if no metrics or filtered data
    else:
        st.markdown("---")
        st.subheader("ğŸ“Š é˜ˆå€¼åˆ†æ")

        enable_threshold_analysis = st.checkbox("å¯ç”¨é˜ˆå€¼åˆ†æ", value=False)

        if enable_threshold_analysis:
            if not selected_metrics_keys:
                st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©è‡³å°‘ä¸€ä¸ªåˆ†ææŒ‡æ ‡ (Yè½´)ã€‚")
            else:
                # Use the first selected metric for threshold analysis
                threshold_metric = selected_metrics_keys[0]
                metric_display_name = question_cols_display_names.get(threshold_metric, threshold_metric)

                st.write(f"å½“å‰åˆ†ææŒ‡æ ‡: **{metric_display_name}**")

                threshold_input = st.text_input(
                    "è¾“å…¥é˜ˆå€¼ï¼ˆç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼š1,2,3ï¼‰",
                    key="threshold_input"
                )

                if threshold_input:
                    try:
                        threshold_values = [float(x.strip()) for x in threshold_input.split(",") if x.strip()]
                        threshold_values.sort()
                    except ValueError:
                        st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼Œç”¨é€—å·åˆ†éš”")

                if not threshold_values:
                    st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªé˜ˆå€¼ã€‚")
                elif threshold_metric not in filtered_df.columns:
                    st.error(f"æ‰€é€‰æŒ‡æ ‡ '{metric_display_name}' åœ¨ç­›é€‰åçš„æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚")
                else:
                    # Create interval labels
                    interval_labels = []
                    for i in range(len(threshold_values) + 1):
                        if i == 0:
                            interval_labels.append(f"< {threshold_values[0]}")
                        elif i == len(threshold_values):
                            interval_labels.append(f">= {threshold_values[-1]}")
                        else:
                            interval_labels.append(f"{threshold_values[i-1]} - {threshold_values[i]}")

                    # Calculate counts for each interval
                    interval_counts = []
                    valid_data = filtered_df[pd.notna(filtered_df[threshold_metric])]

                    for i in range(len(threshold_values) + 1):
                        if i == 0:
                            count = sum(valid_data[threshold_metric] < threshold_values[0])
                        elif i == len(threshold_values):
                            count = sum(valid_data[threshold_metric] >= threshold_values[-1])
                        else:
                            count = sum((valid_data[threshold_metric] >= threshold_values[i-1]) &
                                         (valid_data[threshold_metric] < threshold_values[i]))
                        interval_counts.append(count)

                    # Create DataFrame for visualization
                    total_count = sum(interval_counts)
                    percentages = [count / total_count * 100 if total_count > 0 else 0 for count in interval_counts]

                    threshold_df = pd.DataFrame({
                        'åŒºé—´': interval_labels,
                        'äººæ•°': interval_counts,
                        'ç™¾åˆ†æ¯” (%)': [f"{p:.2f}%" for p in percentages]
                    })

                    # Create two columns for charts
                    col1, col2 = st.columns(2)

                    with col1:
                        st.subheader("æŸ±çŠ¶å›¾")
                        fig_bar = px.bar(
                            threshold_df,
                            x='åŒºé—´',
                            y='äººæ•°',
                            title=f"'{metric_display_name}' çš„é˜ˆå€¼åˆ†æ",
                            text='äººæ•°'
                        )
                        fig_bar.update_layout(
                            xaxis_title="åˆ†æ•°åŒºé—´",
                            yaxis_title="äººæ•°",
                            title_x=0.5
                        )
                        st.plotly_chart(fig_bar, use_container_width=True)

                    with col2:
                        st.subheader("é¥¼å›¾")
                        fig_pie = px.pie(
                            threshold_df,
                            values='äººæ•°',
                            names='åŒºé—´',
                            title=f"'{metric_display_name}' çš„åŒºé—´åˆ†å¸ƒ",
                            hover_data=['ç™¾åˆ†æ¯” (%)']
                        )
                        fig_pie.update_layout(
                            title_x=0.5
                        )
                        st.plotly_chart(fig_pie, use_container_width=True)

                    # Display data table
                    st.subheader("åŒºé—´äººæ•°ç»Ÿè®¡è¡¨")
                    st.dataframe(threshold_df)

                    # Display histogram
                    st.subheader("æ•°æ®åˆ†å¸ƒç›´æ–¹å›¾")

                    # Create histogram with threshold lines
                    fig_hist = px.histogram(
                        valid_data,
                        x=threshold_metric,
                        nbins=20,
                        title=f"'{metric_display_name}' çš„åˆ†å¸ƒç›´æ–¹å›¾",
                        labels={threshold_metric: metric_display_name}
                    )

                    # Add vertical lines for thresholds
                    for threshold in threshold_values:
                        fig_hist.add_vline(
                            x=threshold,
                            line_dash="dash",
                            line_color="red",
                            annotation_text=f"é˜ˆå€¼: {threshold}",
                            annotation_position="top right"
                        )

                    fig_hist.update_layout(
                        xaxis_title=metric_display_name,
                        yaxis_title="é¢‘æ•°",
                        title_x=0.5
                    )

                    st.plotly_chart(fig_hist, use_container_width=True)

                    # Display descriptive statistics
                    st.subheader("æè¿°æ€§ç»Ÿè®¡")

                    # Calculate descriptive statistics
                    desc_stats = valid_data[threshold_metric].describe()

                    # Create a DataFrame for display
                    stats_df = pd.DataFrame({
                        'ç»Ÿè®¡é‡': ['æ ·æœ¬æ•°', 'å¹³å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', '25%åˆ†ä½æ•°', 'ä¸­ä½æ•°', '75%åˆ†ä½æ•°', 'æœ€å¤§å€¼'],
                        'å€¼': [
                            f"{desc_stats['count']:.0f}",
                            f"{desc_stats['mean']:.2f}",
                            f"{desc_stats['std']:.2f}",
                            f"{desc_stats['min']:.2f}",
                            f"{desc_stats['25%']:.2f}",
                            f"{desc_stats['50%']:.2f}",
                            f"{desc_stats['75%']:.2f}",
                            f"{desc_stats['max']:.2f}"
                        ]
                    })

                    st.dataframe(stats_df)

                    # Add download buttons for data
                    col1, col2 = st.columns(2)

                    with col1:
                        # Add download button for threshold analysis data
                        csv_threshold_data = threshold_df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½åŒºé—´ç»Ÿè®¡æ•°æ® (CSV)",
                            data=csv_threshold_data,
                            file_name=f"{threshold_metric}_threshold_analysis.csv",
                            mime='text/csv',
                            key="download_threshold_analysis"
                        )

                    with col2:
                        # Add download button for descriptive statistics
                        csv_stats_data = stats_df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½æè¿°æ€§ç»Ÿè®¡æ•°æ® (CSV)",
                            data=csv_stats_data,
                            file_name=f"{threshold_metric}_descriptive_stats.csv",
                            mime='text/csv',
                            key="download_descriptive_stats"
                        )

                    # --- Category Table Analysis ---
                    st.markdown("---")
                    st.subheader("ğŸ“Š ç±»åˆ«è¡¨æ ¼åˆ†æ")

                    enable_category_analysis = st.checkbox("å¯ç”¨ç±»åˆ«è¡¨æ ¼åˆ†æ", value=False)
                    show_totals = st.checkbox("æ˜¾ç¤ºæ€»è®¡", value=True)

                    if enable_category_analysis:
                        # Select grouping variable for categories
                        category_options = []
                        if 'Region' in filtered_df.columns:
                            category_options.append(('Region', "åœ°åŒº"))
                        if 'gender_str' in filtered_df.columns:
                            category_options.append(('gender_str', "æ€§åˆ«"))
                        if 'native_str' in filtered_df.columns:
                            category_options.append(('native_str', "ä¸Šæµ·äººèº«ä»½"))
                        if 'major_str' in filtered_df.columns and filtered_df['major_str'].nunique() > 0:
                            category_options.append(('major_str', "ä¸“ä¸šç±»å‹"))
                        if 'grade_str' in filtered_df.columns and filtered_df['grade_str'].nunique() > 0:
                            category_options.append(('grade_str', "å¹´çº§"))

                        if not category_options:
                            st.warning("æ²¡æœ‰å¯ç”¨çš„åˆ†ç±»å˜é‡ã€‚")
                        else:
                            # Create a dictionary for the selectbox format_func
                            category_options_dict = {k: v for k, v in category_options}

                            selected_category = st.selectbox(
                                "é€‰æ‹©ç±»åˆ«å˜é‡",
                                options=[k for k, _ in category_options],
                                format_func=lambda x: category_options_dict.get(x, x),
                                key="category_select"
                            )

                            if selected_category:
                                # Get unique categories
                                categories = sorted(valid_data[selected_category].dropna().unique())

                                if len(categories) > 0:
                                    # Allow user to select which categories to include
                                    selected_categories = st.multiselect(
                                        "é€‰æ‹©è¦åŒ…å«çš„ç±»åˆ«",
                                        options=categories,
                                        default=categories,
                                        key="selected_categories"
                                    )

                                    # Add custom group functionality
                                    st.subheader("è‡ªå®šä¹‰èšåˆç¾¤ä½“")
                                    enable_custom_groups = st.checkbox("å¯ç”¨è‡ªå®šä¹‰èšåˆç¾¤ä½“", value=False)

                                    custom_groups = {}
                                    if enable_custom_groups:
                                        st.write("åˆ›å»ºè‡ªå®šä¹‰ç¾¤ä½“ï¼ˆå°†å¤šä¸ªç±»åˆ«èšåˆä¸ºä¸€ä¸ªç¾¤ä½“ï¼‰")

                                        # UI for creating custom groups
                                        col1, col2 = st.columns([1, 2])
                                        with col1:
                                            custom_group_name = st.text_input("ç¾¤ä½“åç§°", key="custom_group_name")
                                        with col2:
                                            group_categories = st.multiselect(
                                                "é€‰æ‹©è¦èšåˆçš„ç±»åˆ«",
                                                options=categories,
                                                key="group_categories"
                                            )

                                        if st.button("æ·»åŠ è‡ªå®šä¹‰ç¾¤ä½“", key="add_custom_group"):
                                            if custom_group_name and group_categories:
                                                custom_groups[custom_group_name] = group_categories
                                                st.success(f"å·²æ·»åŠ è‡ªå®šä¹‰ç¾¤ä½“: {custom_group_name}")

                                        # Display current custom groups
                                        if 'custom_groups' in st.session_state:
                                            for name, cats in st.session_state.custom_groups.items():
                                                st.write(f"- {name}: {', '.join(cats)}")

                                        # Store custom groups in session state
                                        if custom_groups:
                                            if 'custom_groups' not in st.session_state:
                                                st.session_state.custom_groups = {}
                                            st.session_state.custom_groups.update(custom_groups)

                                    # Get custom groups from session state
                                    if 'custom_groups' in st.session_state:
                                        custom_groups = st.session_state.custom_groups

                                    if selected_categories or custom_groups:
                                        # Create interval labels (same as before)
                                        interval_labels = []
                                        for i in range(len(threshold_values) + 1):
                                            if i == 0:
                                                interval_labels.append(f"< {threshold_values[0]}")
                                            elif i == len(threshold_values):
                                                interval_labels.append(f">= {threshold_values[-1]}")
                                            else:
                                                interval_labels.append(f"{threshold_values[i-1]} - {threshold_values[i]}")

                                        # Get all categories that need to be included (selected categories + all categories in any custom group)
                                        categories_to_include = set(selected_categories) if selected_categories else set()
                                        for group_name, group_cats in custom_groups.items():
                                            categories_to_include.update(group_cats)

                                        # Filter data to include all necessary categories
                                        category_data = valid_data[valid_data[selected_category].isin(categories_to_include)] if categories_to_include else valid_data

                                        # Combine selected categories with custom groups
                                        all_categories = list(selected_categories) if selected_categories else []
                                        for group_name in custom_groups:
                                            if group_name not in all_categories:
                                                all_categories.append(group_name)

                                        # Create contingency table with categories as rows and intervals as columns
                                        contingency_table = np.zeros((len(all_categories), len(interval_labels)))

                                        # Fill contingency table
                                        for i, category in enumerate(all_categories):
                                            # Check if this is a custom group
                                            if category in custom_groups:
                                                # Combine data from multiple categories
                                                group_categories = custom_groups[category]
                                                category_subset = category_data[category_data[selected_category].isin(group_categories)]
                                            else:
                                                # Regular category
                                                category_subset = category_data[category_data[selected_category] == category]

                                            for j, interval in enumerate(interval_labels):
                                                if j == 0:  # First interval
                                                    count = sum(category_subset[threshold_metric] < threshold_values[0])
                                                elif j == len(threshold_values):  # Last interval
                                                    count = sum(category_subset[threshold_metric] >= threshold_values[-1])
                                                else:  # Middle intervals
                                                    count = sum((category_subset[threshold_metric] >= threshold_values[j-1]) &
                                                                (category_subset[threshold_metric] < threshold_values[j]))

                                                contingency_table[i, j] = count

                                        # Calculate row totals (for categories)
                                        category_totals = np.sum(contingency_table, axis=1)

                                        # Calculate column totals (for intervals)
                                        interval_totals = np.sum(contingency_table, axis=0)

                                        # Perform chi-square test if we have enough data
                                        if np.sum(contingency_table) > 0 and np.all(category_totals > 0) and np.all(interval_totals > 0):
                                            chi2, p, dof, expected = chi2_contingency(contingency_table)
                                            chi2_result = f"Ï‡Â² = {chi2:.2f}, p = {p:.4f}"
                                        else:
                                            chi2_result = "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¡æ–¹æ£€éªŒ"

                                        # Create DataFrame for display with categories as rows and intervals as columns
                                        category_table = pd.DataFrame(contingency_table, index=all_categories, columns=interval_labels)

                                        # Add row totals (for categories) if show_totals is checked
                                        if show_totals:
                                            category_table['æ€»è®¡'] = category_totals

                                            # Add column totals (for intervals)
                                            category_table.loc['æ€»è®¡'] = list(interval_totals) + [np.sum(contingency_table)]

                                        # Always add chi-square test results regardless of show_totals
                                        # Ensure the chi-square test row has the same number of columns as the table
                                        empty_values = [''] * (len(category_table.columns) - 1)
                                        category_table.loc['å¡æ–¹æ£€éªŒ'] = [chi2_result] + empty_values

                                        # Display the table
                                        st.subheader(f"æŒ‰ {category_options_dict.get(selected_category, selected_category)} åˆ†ç±»çš„åŒºé—´ç»Ÿè®¡è¡¨")
                                        st.dataframe(category_table)

                                        # Add download button for category table
                                        csv_category_table = category_table.to_csv(index=True).encode('utf-8-sig')
                                        st.download_button(
                                            label=f"ğŸ“¥ ä¸‹è½½ç±»åˆ«è¡¨æ ¼æ•°æ® (CSV)",
                                            data=csv_category_table,
                                            file_name=f"{threshold_metric}_category_analysis.csv",
                                            mime='text/csv',
                                            key="download_category_analysis"
                                        )
                                    else:
                                        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç±»åˆ«æˆ–åˆ›å»ºè‡ªå®šä¹‰èšåˆç¾¤ä½“ã€‚")
                                else:
                                    st.warning(f"é€‰å®šçš„ç±»åˆ«å˜é‡ '{category_options_dict.get(selected_category, selected_category)}' æ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")

# --- Encoding Information ---
with st.expander("â„¹ï¸ æŸ¥çœ‹ç¼–ç è¯´æ˜å’ŒåŸå§‹é—®å·ä¿¡æ¯"):
    st.markdown("#### **é—®å·é—®é¢˜ä¸ç¼–ç åå˜é‡åæ˜ å°„**")
    rename_df_display = pd.DataFrame(list(rename_map.items()), columns=['åŸå§‹é—®å·é—®é¢˜', 'ç¼–ç åå˜é‡å'])
    st.table(rename_df_display)

    st.markdown("#### **å…·ä½“ç¼–ç è¯¦æƒ…**")

    st.markdown("**æ€§åˆ« (gender_str / gender_code):**")
    gender_df_disp = pd.DataFrame({'åŸé—®å·ç­”æ¡ˆ': ['A.ç”·', 'B.å¥³'], 'ç¼–ç å€¼ (gender_code)': [1,0], 'åˆ†æç”¨æ ‡ç­¾ (gender_str)': ['ç”·','å¥³']})
    st.table(gender_df_disp)

    st.markdown("**ä¸Šæµ·äººèº«ä»½ (native_str):**")
    # native_mapping_display from load_and_process_data
    native_df_disp = pd.DataFrame(list(native_map_disp.items()), columns=['åŸé—®å·ç­”æ¡ˆ', 'åˆ†æç”¨æ ‡ç­¾'])
    st.table(native_df_disp)

    st.markdown("**ä¸“ä¸šç±»å‹ (major_str / major):**")
    if major_mapping:
        major_df_disp = pd.DataFrame(list(major_mapping.items()), columns=['åŸé—®å·ç­”æ¡ˆ', 'ç¼–ç å€¼ (major)'])
        st.table(major_df_disp.sort_values(by='ç¼–ç å€¼ (major)'))
    else:
        st.markdown("_æ— ä¸“ä¸šç±»å‹æ•°æ®æˆ–æ˜ å°„_")

    st.markdown("**å¹´çº§ (grade_str / grade):**")
    if grade_mapping:
        grade_df_disp = pd.DataFrame(list(grade_mapping.items()), columns=['åŸé—®å·ç­”æ¡ˆ', 'ç¼–ç å€¼ (grade)'])
        st.table(grade_df_disp.sort_values(by='ç¼–ç å€¼ (grade)'))
    else:
        st.markdown("_æ— å¹´çº§æ•°æ®æˆ–æ˜ å°„_")

    course_attitude_map_disp = {'A.éå¸¸æ”¯æŒ':4, 'B.æ”¯æŒ':3, 'C.æ— æ‰€è°“':2, 'D.åå¯¹':1}
    st.markdown("**å¤§å­¦å¼€è®¾ä¸Šæµ·è¯è¯¾ç¨‹æ€åº¦ (course_attitude):**")
    st.table(pd.DataFrame(list(course_attitude_map_disp.items()), columns=['åŸé—®å·ç­”æ¡ˆ', 'ç¼–ç å€¼']))

    identity_map_disp = {'A.æ˜¯çš„':4, 'B.éƒ¨åˆ†æ˜¯':3, 'D.ä¸æ¸…æ¥š':2, 'C.å¦':1}
    st.markdown("**å­¦ä¹ /ä¼šè¯´ä¸Šæµ·è¯æ˜¯â€˜æœ¬åœ°èº«ä»½â€™è±¡å¾ (identity):**")
    st.table(pd.DataFrame(list(identity_map_disp.items()), columns=['åŸé—®å·ç­”æ¡ˆ', 'ç¼–ç å€¼']))

    st.markdown("**çœä»½ä¸åœ°åŒºå¯¹åº” (Region):**")
    st.table(pd.DataFrame(list(prov_to_region_map.items()), columns=['çœä»½', 'åœ°åŒº']))


st.sidebar.markdown("---")
st.sidebar.info("""
**ğŸ’¡ è¿è¡ŒæŒ‡å—:**
1. ç¡®ä¿ `data.csv` æ–‡ä»¶ä¸æ­¤ `interactive_app.py` åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚
2. å®‰è£…å¿…è¦çš„åº“: 
   `pip install streamlit pandas plotly openpyxl category_encoders`
3. åœ¨ç»ˆç«¯ä¸­è¿è¡Œ: 
   `streamlit run interactive_app.py`
""")


st.markdown("---")
st.markdown("Shanghainese Dialect Survey Interactive Analysis | Developed with Streamlit") 
